{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata, FunctionTool\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "\n",
    "#https://docs.llamaindex.ai/en/stable/getting_started/starter_example_local/\n",
    "\n",
    "# setup promptTemplate for wrapping the prompt\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "from llama_index.vector_stores.redis import RedisVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Prevents CUDA OOM errors\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents_anayurt = SimpleDirectoryReader(\"./data/d1/\").load_data()\n",
    "documents_wiki = SimpleDirectoryReader(\"./data/d2/\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\n",
    "    \"Below is an instruction that describes a task. \"\n",
    "    \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. .\\n\\n\"\n",
    "    \"### Instruction:\\n{query_str}\\n\\n### Response:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={ \"do_sample\": True, \"temperature\":0.7, \"top_p\":0.95, \"top_k\":40,\"repetition_penalty\":1.1},\n",
    "    #query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"TheBloke/Llama-2-7B-Chat-GPTQ\",\n",
    "    model_name=\"TheBloke/Llama-2-7B-Chat-GPTQ\",\n",
    "    device_map=\"auto\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    # uncomment this if using CUDA to reduce memory usage (Untested)\n",
    "    #model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "\n",
    "Settings.chunk_size = 512\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector_store_d1 = RedisVectorStore(\n",
    "    index_name=\"d1\",\n",
    "    index_prefix=\"d1\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "vector_store_d2 = RedisVectorStore(\n",
    "    index_name=\"d2\",\n",
    "    index_prefix=\"d2\",\n",
    "    redis_url=\"redis://localhost:6379\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "d1_storage_context = StorageContext.from_defaults(vector_store=vector_store_d1)\n",
    "\n",
    "d2_storage_context = StorageContext.from_defaults(vector_store=vector_store_d2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_d1 = VectorStoreIndex.from_documents(\n",
    "    documents_anayurt, storage_context=d1_storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "index_d2 = VectorStoreIndex.from_documents(\n",
    "    documents_wiki, storage_context=d2_storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "query_engine_d1 = index_d1.as_query_engine()\n",
    "\n",
    "query_engine_d2 = index_d2.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "print(len(embeddings))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream_anayurt = query_engine_d1.query(\"What can you tell me about the Jeff?\")\n",
    "\n",
    "# can be slower to start streaming since llama-index often involves many LLM calls\n",
    "print(response_stream_anayurt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_stream_wiki = query_engine_d2.query(\"What can you tell me about the Jeff?\")\n",
    "\n",
    "# can be slower to start streaming since llama-index often involves many LLM calls\n",
    "print(response_stream_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Function to send a message to Discord using the Discord webhook URL: https://discordapp.com/api/webhooks/.....\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def send_discord_message(message):\n",
    "    url = \"https://discordapp.com/api/webhooks/...\n",
    "    data = {}\n",
    "    data[\"content\"] = message\n",
    "    result = requests.post(url, data=json.dumps(data), headers={\"Content-Type\": \"application/json\"})\n",
    "    try:\n",
    "        result.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(err)\n",
    "    else:\n",
    "        print(\"Payload delivered successfully, code {}.\".format(result.status_code))\n",
    "\n",
    "\n",
    "send_discord_message(\"Hello World!\")\n",
    "\n",
    "\n",
    "# Explain the above function in JSON format\n",
    "\n",
    "explain_send_discord_message = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"message\": {\n",
    "        \"type\": \"string\",\n",
    "        \"description\": \"The message to send to Discord\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"message\"],\n",
    "    }\n",
    "\n",
    "\n",
    "discordFunctionTool = FunctionTool(send_discord_message, {\n",
    "  \"name\": \"Send_Discord_Message\",\n",
    "  \"description\": \"Use this function to send messages to Discord using the Discord webhook URL.\",\n",
    "  \"parameters\": explain_send_discord_message,\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine_d1,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"d1_query_engine_tool\",\n",
    "            description=(\n",
    "                \"Use this tool to answer questions about the Billionaire Jeff Bezos.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine_d2,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"d2_query_engine_tool\",\n",
    "            description=(\n",
    "                \"Use this tool to answer questions about the Billionaire Jeff Epstein.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core.selectors import LLMSingleSelector, LLMMultiSelector\n",
    "\n",
    "query_engine = RouterQueryEngine.from_defaults(query_engine_tools=tools, selector=LLMSingleSelector.from_defaults())\n",
    "\n",
    "response = query_engine.query(\"When was Jeff Bezos born?\")\n",
    "\n",
    "print(str(response))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
